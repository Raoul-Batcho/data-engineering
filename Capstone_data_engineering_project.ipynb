{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "tags": []
   },
   "source": [
    "# Project Title: Cities demographics, Immigration and airports\n",
    "### Data Engineering Capstone Project  \n",
    "\n",
    "#### Project Summary\n",
    "Capstone, a startup has grown their data warehouse and want to move their data in a data lake. Their data resides in S3, in a directory of parquet on immigration events, as well as two directories with CSV files on US cities demographics and airport codes respectively .\n",
    "\n",
    "We are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables and fact table. This will allow their analytics team to continue finding insights such as the date, duration of immigration in terms of number of days, weeks, months, years for example, immigrants data such as the visa type, the mode of immigration and cities demographics and the airports in the respective cities as well.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:12:29.378205Z",
     "iopub.status.busy": "2022-03-13T07:12:29.377975Z",
     "iopub.status.idle": "2022-03-13T07:12:59.272029Z",
     "shell.execute_reply": "2022-03-13T07:12:59.271469Z",
     "shell.execute_reply.started": "2022-03-13T07:12:29.378178Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faec7bd378d64b0f815bc69d2cc34dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1647154920989_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-36-223.ec2.internal:20888/proxy/application_1647154920989_0002/\" class=\"emr-proxy-link\" emr-resource=\"j-KWW0TF9V3J41\n",
       "\" application-id=\"application_1647154920989_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-47-233.ec2.internal:8042/node/containerlogs/container_1647154920989_0002_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:13:29.350974Z",
     "iopub.status.busy": "2022-03-13T07:13:29.350747Z",
     "iopub.status.idle": "2022-03-13T07:13:29.626431Z",
     "shell.execute_reply": "2022-03-13T07:13:29.625689Z",
     "shell.execute_reply.started": "2022-03-13T07:13:29.350949Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a5da6d187f4a6fb036f410f087ae19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime, timedelta \n",
    "import os\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope of the Project and Data Gathering\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The project consist in building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables and fact table.\n",
    "For this purpose, We are going to execute the following tasks:\n",
    "* Build a data model base on star schema for analytics processes efficiency.\n",
    "* Create a spark session.\n",
    "* Four functions are built to run ETL respectively for three dimension tables :\n",
    "    process_cities_data()\n",
    "    process_airport_data()\n",
    "    process_immigrant_data()\n",
    "  and one fact table:\n",
    "    process_immigration_data() \n",
    "* Access S3 using AWS credentials.\n",
    "* Run a pipline containing the functions mentionned above and load the data back into S3 as a set of dimensional tables and fact table\n",
    "* Perform data quality check.\n",
    "\n",
    "\n",
    "#### Data description and Gathering\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "#### I94 Immigration Data:\n",
    "This data comes from the US National Tourism and Trade Office.In the past all foreign visitors to the U.S. arriving via air or sea were required to complete paper Customs and Border Protection Form I-94 Arrival/Departure Record or Form I-94W Nonimmigrant Visa Waiver Arrival/Departure Record and this dataset comes from this forms. Contains SAS format data.\n",
    "\n",
    "#### U.S. City Demographic Data: \n",
    "This data comes from OpenSoft.This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. Contains CSV format data.\n",
    "\n",
    "#### Airport Code Table: \n",
    "This is a simple table of airport codes and corresponding cities. Contains CSV format data.\n",
    "\n",
    "### Tools\n",
    "\n",
    "#### Python: \n",
    "* For data processing\n",
    "\n",
    "#### Pandas: \n",
    "* For data exploratory, data analysis on small data set\n",
    "\n",
    "#### AWS S3: data storage\n",
    "\n",
    "#### PySpark: \n",
    "* For data processing on large data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:13:42.170823Z",
     "iopub.status.busy": "2022-03-13T07:13:42.170488Z",
     "iopub.status.idle": "2022-03-13T07:13:49.535584Z",
     "shell.execute_reply": "2022-03-13T07:13:49.534639Z",
     "shell.execute_reply.started": "2022-03-13T07:13:42.170783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0b8c04622f467993fb595b5a348a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in the immigration data \n",
    "df_spark_immigration=spark.read.parquet(\"s3://raoul-bucket/sas_data\") \n",
    "\n",
    "# create temporary table\n",
    "immigrant_table = df_spark_immigration.createOrReplaceTempView(\"immigrant_table\")\n",
    "\n",
    "# create temporary table\n",
    "immigration_table = df_spark_immigration.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:13:54.104490Z",
     "iopub.status.busy": "2022-03-13T07:13:54.104227Z",
     "iopub.status.idle": "2022-03-13T07:13:54.381723Z",
     "shell.execute_reply": "2022-03-13T07:13:54.381053Z",
     "shell.execute_reply.started": "2022-03-13T07:13:54.104463Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe2fdb202d24ecaaf415b8fb8e6dd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Customization of the cities data schema\n",
    "df_spark_cities_schema=StructType([\n",
    "                       StructField(\"city\",StringType()),\n",
    "                       StructField(\"state_name\",StringType()),\n",
    "                       StructField(\"median_age\",DoubleType()),\n",
    "                       StructField(\"male_population\",IntegerType()),\n",
    "                       StructField(\"female_population\",IntegerType()),\n",
    "                       StructField(\"total_population\",IntegerType()),\n",
    "                       StructField(\"number_veterans\",IntegerType()),\n",
    "                       StructField(\"foreign_born\",IntegerType()),\n",
    "                       StructField(\"avg_household_size\",DoubleType()),\n",
    "                       StructField(\"state_code\",StringType()),\n",
    "                       StructField(\"race\",StringType()),\n",
    "                       StructField(\"race_count\",IntegerType())])\n",
    "\n",
    "# Read in the cities data \n",
    "df_spark_cities=spark.read.csv(\"s3://raoul-bucket/us-cities-demographics.csv\", schema=df_spark_cities_schema, sep=\";\", mode=\"DROPMALFORMED\")\n",
    "\n",
    "# create temporary table\n",
    "cities_table=df_spark_cities.createOrReplaceTempView(\"cities_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:13:56.261189Z",
     "iopub.status.busy": "2022-03-13T07:13:56.260952Z",
     "iopub.status.idle": "2022-03-13T07:13:59.575540Z",
     "shell.execute_reply": "2022-03-13T07:13:59.574832Z",
     "shell.execute_reply.started": "2022-03-13T07:13:56.261164Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b697c474f0a497eb0de9cc87d3b09a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in the airport data\n",
    "df_spark_airport = spark.read.csv(\"s3://raoul-bucket/airport-codes_csv.csv\", sep=\",\", header=True)\n",
    "\n",
    "# put coordinates column in first normal form\n",
    "df_spark_airport = df_spark_airport.withColumn(\"latitude\", split(df_spark_airport[\"coordinates\"], \", \").getItem(0).cast(\"double\"))\\\n",
    "                                           .withColumn(\"longitude\", split(df_spark_airport[\"coordinates\"], \", \").getItem(1).cast(\"double\"))\\\n",
    "                                           .withColumn(\"iso_region\", substring(df_spark_airport[\"iso_region\"], 4,2))\\\n",
    "                                           .drop(\"coordinates\")  \n",
    "# create temporary table\n",
    "airport_table = df_spark_airport.createOrReplaceTempView(\"airport_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:13:59.576872Z",
     "iopub.status.busy": "2022-03-13T07:13:59.576676Z",
     "iopub.status.idle": "2022-03-13T07:13:59.642860Z",
     "shell.execute_reply": "2022-03-13T07:13:59.642106Z",
     "shell.execute_reply.started": "2022-03-13T07:13:59.576849Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7f0edc89304b2fa68875a5c44ee00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " output_bucket = \"s3a://raoul-bucket/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Data exploration \n",
    "Identification of data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    " Cleaning tasks for cities data. The steps involved are the following:\n",
    "##### 1- Select valuable columns\n",
    "##### 2- Drop duplicate entries\n",
    "##### 3- Drop lines containing Null value\n",
    "##### 4- Put the table at the first normal form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:14:08.117003Z",
     "iopub.status.busy": "2022-03-13T07:14:08.116623Z",
     "iopub.status.idle": "2022-03-13T07:14:08.189589Z",
     "shell.execute_reply": "2022-03-13T07:14:08.188795Z",
     "shell.execute_reply.started": "2022-03-13T07:14:08.116949Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9dd5ed0ec34c54b793b3f58d832b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning tasks for cities data. The steps involved are the following:\n",
    "# 1- Select valuable columns\n",
    "# 2- Drop duplicate entries\n",
    "# 3- Drop lines containing Null value\n",
    "# 4- Put the table at the first normal form\n",
    "\n",
    "def process_cities_data():\n",
    "    \"\"\"\n",
    "       The function:\n",
    "       Extracts data from us-cities-demographic.csv \n",
    "       Transforms the data and create cities_table \n",
    "       Loads cities_table back to S3 in parquet format\n",
    "    \"\"\"\n",
    "    \n",
    "       # Create cities_table \n",
    "    cities_table=spark.sql(\"\"\"SELECT md5(city||state_name) city_id,\n",
    "                                        city,\n",
    "                                        state_name,\n",
    "                                        state_code,\n",
    "                                        median_age,\n",
    "                                        male_population,\n",
    "                                        female_population,\n",
    "                                        total_population                               \n",
    "                                 FROM  cities_table \n",
    "                                 ORDER BY city \n",
    "                                      \"\"\").dropDuplicates().dropna()\n",
    "    \n",
    "    # load cities_table to s3 in parquet format\n",
    "    cities_table.write.partitionBy(\"state_code\",\"city\").mode(\"append\").parquet(output_bucket+'us_cities_demographics.parquet')\n",
    "    \n",
    "    cities_table.show(n=3)\n",
    "    return cities_table.count()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:14:11.493383Z",
     "iopub.status.busy": "2022-03-13T07:14:11.493034Z",
     "iopub.status.idle": "2022-03-13T07:14:11.578334Z",
     "shell.execute_reply": "2022-03-13T07:14:11.577664Z",
     "shell.execute_reply.started": "2022-03-13T07:14:11.493343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5946a85f0bff486d9b8f2bb9bfc231dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning tasks for airport data and creation of airport dimension table\n",
    "# 1- Select valuable columns\n",
    "# 2- Drop duplicate entries\n",
    "# 3- Drop lines containing Null value\n",
    "# 4- Put the table at the first normal form\n",
    "\n",
    "def process_airport_data():\n",
    "        \"\"\"\n",
    "            The function:\n",
    "            Extracts data from airport-codes.csv \n",
    "            Transforms the data and create airport_table \n",
    "            Loads airport_table back to S3 in parquet format.\n",
    "        \"\"\"\n",
    "        # Create airport_table\n",
    "        \n",
    "        airport_table = spark.sql(\"\"\"SELECT ident airport_id,\n",
    "                                            type airport_type,\n",
    "                                            name airport_name,\n",
    "                                            int(elevation_ft) elevation_ft,\n",
    "                                            iso_region state_code,\n",
    "                                            municipality city,\n",
    "                                            latitude,\n",
    "                                            longitude\n",
    "                                      \n",
    "                                     FROM   airport_table\n",
    "                                     WHERE type != \"closed\"\n",
    "                                     ORDER BY airport_id\n",
    "                                     \n",
    "                                  \"\"\").dropDuplicates().dropna()\n",
    "        \n",
    "        # load the table in s3 under parquet format\n",
    "        airport_table.write.partitionBy(\"state_code\",\"city\").mode(\"append\").parquet(output_bucket+'airports_data.parquet')\n",
    "        \n",
    "        airport_table.show(n=3)\n",
    "        return airport_table.count()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:14:14.624048Z",
     "iopub.status.busy": "2022-03-13T07:14:14.623805Z",
     "iopub.status.idle": "2022-03-13T07:14:14.686209Z",
     "shell.execute_reply": "2022-03-13T07:14:14.685575Z",
     "shell.execute_reply.started": "2022-03-13T07:14:14.624024Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9def50339e45328ab10fcaacd8f726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning tasks for immigration data and creation of immigrant dimension table\n",
    "# 1- Select valuable columns\n",
    "# 2- Drop duplicate entries\n",
    "# 3- Drop lines containing Null value\n",
    "# 4- Put the table at the first normal form\n",
    "\n",
    "def process_immigrant_data():\n",
    "        \"\"\"\n",
    "            The function:\n",
    "            Extracts data from SAS_data \n",
    "            Transforms the data and create immigrant_table \n",
    "            Loads immigrant_table back to S3 in parquet format.\n",
    "        \"\"\"\n",
    "      # Create immigrant_table    \n",
    "        immigrant_table=spark.sql(\"\"\" SELECT int(cicid) imm_id,\n",
    "                                           int(i94bir) age,\n",
    "                                           biryear birth_year,\n",
    "                                           gender,\n",
    "                                           visatype visa_type,\n",
    "                                           i94addr state_code\n",
    "                                    FROM immigrant_table\n",
    "                                 \"\"\").dropDuplicates().dropna()\n",
    "        \n",
    "        # load the table in s3 under parquet format\n",
    "        immigrant_table.write.mode(\"append\").parquet(output_bucket+'immigrants_data.parquet')\n",
    "        \n",
    "        immigrant_table.show(n=3)\n",
    "        return immigrant_table.count()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:14:18.886986Z",
     "iopub.status.busy": "2022-03-13T07:14:18.886735Z",
     "iopub.status.idle": "2022-03-13T07:14:18.949590Z",
     "shell.execute_reply": "2022-03-13T07:14:18.948892Z",
     "shell.execute_reply.started": "2022-03-13T07:14:18.886948Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6054f2dfbf4335a9c73071c9c6adab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function converts SAS date format to datetime\n",
    "def sas_to_datetime(date):\n",
    "    if date is not None:\n",
    "        return pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    \n",
    "sas_to_datetime_udf = udf(sas_to_datetime, DateType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:14:23.968591Z",
     "iopub.status.busy": "2022-03-13T07:14:23.968315Z",
     "iopub.status.idle": "2022-03-13T07:14:24.038829Z",
     "shell.execute_reply": "2022-03-13T07:14:24.038252Z",
     "shell.execute_reply.started": "2022-03-13T07:14:23.968565Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0274f771c98426298ec2743f1957438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performing cleaning tasks for immigration data and creation of immigration Fact table\n",
    "# 1- Select valuable columns\n",
    "# 2- Drop duplicate entries\n",
    "# 3- Drop lines containing Null value\n",
    "# 4- Put the table at the first normal form\n",
    "\n",
    "def process_immigration_data():\n",
    "        \"\"\"\n",
    "            The function:\n",
    "            Extracts data from SAS_data \n",
    "            Transforms the data and create immigration_table \n",
    "            Loads immigration_table back to S3 in parquet format.\n",
    "        \"\"\"\n",
    "                   \n",
    "       # Create immigration_table \n",
    "        immigration_table = spark.sql(\"\"\"SELECT int(immigration_table.cicid) imm_id,\n",
    "                                               airport_table.ident airport_id,                                     \n",
    "                                               md5(city||state_name) city_id,                                     \n",
    "                                               int(immigration_table.arrdate) arrival_date,\n",
    "                                               int(immigration_table.depdate) departure_date,\n",
    "                                               CASE WHEN int(immigration_table.i94mode)=1 THEN \"air\"\n",
    "                                                    WHEN int(immigration_table.i94mode)=2 THEN \"sea\"\n",
    "                                                    WHEN int(immigration_table.i94mode)=3 THEN \"Land\" \n",
    "                                                    ELSE \"Not reported\" \n",
    "                                                    END as mode,\n",
    "                                               immigration_table.i94addr state_code,\n",
    "                                               int(immigration_table.i94yr) immigration_year,\n",
    "                                               int(immigration_table.i94mon) immigration_month,\n",
    "                                               airport_table.latitude,\n",
    "                                               airport_table.longitude                                               \n",
    "                                                                           \n",
    "                                          FROM  immigration_table\n",
    "                                          JOIN  immigrant_table\n",
    "                                          ON    immigration_table.cicid=immigrant_table.cicid\n",
    "                                          JOIN  airport_table\n",
    "                                          ON    immigrant_table.i94addr=airport_table.iso_region\n",
    "                                          JOIN  cities_table\n",
    "                                          ON    airport_table.municipality=cities_table.City AND airport_table.iso_region=cities_table.state_code                           \n",
    "                               \n",
    "                                        \"\"\").dropDuplicates().dropna()\n",
    "    \n",
    "       # Convert SAS arrival_date and departure_date columns to Datetime format \n",
    "        immigration_table=immigration_table.withColumn(\"arrival_date\", sas_to_datetime_udf(\"arrival_date\"))\n",
    "        immigration_table=immigration_table.withColumn(\"departure_date\", sas_to_datetime_udf(\"departure_date\"))\n",
    "        \n",
    "       # Create the stay_duration_in_days column \n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_days\", datediff(col(\"departure_date\"),col(\"arrival_date\")))\n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_days\", round(immigration_table[\"stay_duration_in_days\"], scale=0))\n",
    "    \n",
    "       # Create the stay_duration_in_weeks column\n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_weeks\", datediff(col(\"departure_date\"),col(\"arrival_date\"))*52/365)\n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_weeks\", round(immigration_table[\"stay_duration_in_weeks\"], scale=1))\n",
    "        \n",
    "       # Create the stay_duration_in_months column\n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_months\", months_between(col(\"departure_date\"),col(\"arrival_date\")))\n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_months\", round(immigration_table[\"stay_duration_in_months\"], scale=1))\n",
    "        \n",
    "       # Create the stay_duration_in_years column \n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_years\", datediff(col(\"departure_date\"),col(\"arrival_date\"))/365)\n",
    "        immigration_table=immigration_table.withColumn(\"stay_duration_in_years\", round(immigration_table[\"stay_duration_in_years\"], scale=1))\n",
    "        \n",
    "       # load the table in s3 under parquet format\n",
    "        immigration_table.write.partitionBy(\"immigration_year\",\"immigration_month\").mode(\"append\").parquet(output_bucket+'immigration_data.parquet')\n",
    "        \n",
    "        immigration_table.show(n=3)\n",
    "        return immigration_table.count()\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "#### Dimension tables:\n",
    "##### cities_table\n",
    "           city_id    PK\n",
    "           city\n",
    "           state_name\n",
    "           state_code\n",
    "           median_age\n",
    "           male_population\n",
    "           female_population\n",
    "           total_population\n",
    "           \n",
    "##### airport_table\n",
    "           airport_id     PK\n",
    "           airport_type\n",
    "           airport_name\n",
    "           elevation_ft\n",
    "           state_code\n",
    "           city\n",
    "           latitude\n",
    "           longitude \n",
    "           \n",
    "##### immigrant_table\n",
    "           imm_id         PK\n",
    "           age\n",
    "           birth_year\n",
    "           gender\n",
    "           visa_type\n",
    "           state_code\n",
    "           \n",
    "#### Fact table:\n",
    "##### immigration_table\n",
    "           imm_id          PK\n",
    "           airport_id      FK\n",
    "           city_id         FK\n",
    "           arrival_date\n",
    "           departure_date\n",
    "           mode\n",
    "           state_code\n",
    "           immigration_year\n",
    "           immigration_month\n",
    "           stay_duration_in_days\n",
    "           stay_duration_in_weeks\n",
    "           stay_duration_in_months\n",
    "           stay_duration_in_years\n",
    "\n",
    "                         \n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The data is modelized by the following functions which take two parameters each:\n",
    "  The staging dataframe and the output.\n",
    "* process_cities_data(df_spark_cities, output_data)\n",
    "* process_airport_data(df_spark_airport, output_data)\n",
    "* process_immigrant_data(df_spark_immigration, output_data)\n",
    "* process_immigration_data(df_spark_immigration, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T07:14:41.198935Z",
     "iopub.status.busy": "2022-03-13T07:14:41.198636Z",
     "iopub.status.idle": "2022-03-13T09:21:10.504089Z",
     "shell.execute_reply": "2022-03-13T09:21:10.503235Z",
     "shell.execute_reply.started": "2022-03-13T07:14:41.198900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5f594225ab4ccdb90bbe1692d8f3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+----------+----------+---------------+-----------------+----------------+\n",
      "|             city_id|       city|state_name|state_code|median_age|male_population|female_population|total_population|\n",
      "+--------------------+-----------+----------+----------+----------+---------------+-----------------+----------------+\n",
      "|2d0c364938ff09a61...|Bloomington|  Illinois|        IL|      35.1|          37972|            40323|           78295|\n",
      "|1f2154b1c8397d311...|    Kenosha| Wisconsin|        WI|      36.3|          49349|            50507|           99856|\n",
      "|501c7d03a03961d9a...|Cheektowaga|  New York|        NY|      40.7|          37476|            38599|           76075|\n",
      "+--------------------+-----------+----------+----------+----------+---------------+-----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "cities data ETL done.\n",
      "+----------+-------------+--------------------+------------+----------+-----------+------------------+------------------+\n",
      "|airport_id| airport_type|        airport_name|elevation_ft|state_code|       city|          latitude|         longitude|\n",
      "+----------+-------------+--------------------+------------+----------+-----------+------------------+------------------+\n",
      "|      0CD1|     heliport|Colorado Plains M...|        4356|        CO|Fort Morgan|      -103.7963389|        40.2610917|\n",
      "|      2AR4|small_airport|        Jaynes Field|         212|        AR|    Trumann|-90.52780151367188| 35.67070007324219|\n",
      "|      34TS|small_airport| Canyon Lake Airport|         940|        TX|Canyon Lake|-98.24749755859375|29.915800094604492|\n",
      "+----------+-------------+--------------------+------------+----------+-----------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Airports data ETL done.\n",
      "+------+---+----------+------+---------+----------+\n",
      "|imm_id|age|birth_year|gender|visa_type|state_code|\n",
      "+------+---+----------+------+---------+----------+\n",
      "|460121| 49|    1967.0|     M|       WT|        CT|\n",
      "|461182| 34|    1982.0|     F|       WT|        FL|\n",
      "|462267| 51|    1965.0|     M|       WB|        TX|\n",
      "+------+---+----------+------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "immigrants data ETL done.\n",
      "+-------+----------+--------------------+------------+--------------+----+----------+----------------+-----------------+------------------+------------------+---------------------+----------------------+-----------------------+----------------------+\n",
      "| imm_id|airport_id|             city_id|arrival_date|departure_date|mode|state_code|immigration_year|immigration_month|          latitude|         longitude|stay_duration_in_days|stay_duration_in_weeks|stay_duration_in_months|stay_duration_in_years|\n",
      "+-------+----------+--------------------+------------+--------------+----+----------+----------------+-----------------+------------------+------------------+---------------------+----------------------+-----------------------+----------------------+\n",
      "|6051842|      00FD|239a1b21f5bf8dab2...|  2016-04-11|    2016-05-13| air|        FL|            2016|                4|-82.34539794921875|28.846599578857422|                   32|                   4.6|                    1.1|                   0.1|\n",
      "|5817380|      00FD|239a1b21f5bf8dab2...|  2016-04-30|    2016-05-02| air|        FL|            2016|                4|-82.34539794921875|28.846599578857422|                    2|                   0.3|                    0.1|                   0.0|\n",
      "|5880711|      00FD|239a1b21f5bf8dab2...|  2016-04-30|    2016-05-04| air|        FL|            2016|                4|-82.34539794921875|28.846599578857422|                    4|                   0.6|                    0.2|                   0.0|\n",
      "+-------+----------+--------------------+------------+--------------+----+----------+----------------+-----------------+------------------+------------------+---------------------+----------------------+-----------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "immigration data ETL done."
     ]
    }
   ],
   "source": [
    "# This function triggers the ETL process on the Three datasets, create the star schema data model and load the Fact and dimension tables in to S3  \n",
    "def pipeline():\n",
    "    \n",
    "    count_cities_table=process_cities_data()\n",
    "    print(\"cities data ETL done.\")\n",
    "    \n",
    "    count_airport_table=process_airport_data()\n",
    "    print(\"Airports data ETL done.\")\n",
    "    \n",
    "    count_immigrant_table=process_immigrant_data()\n",
    "    print(\"immigrants data ETL done.\")\n",
    "    \n",
    "    count_immigration_table=process_immigration_data()\n",
    "    print(\"immigration data ETL done.\")\n",
    "    \n",
    "pipeline() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T09:22:05.909353Z",
     "iopub.status.busy": "2022-03-13T09:22:05.909123Z",
     "iopub.status.idle": "2022-03-13T09:22:11.243759Z",
     "shell.execute_reply": "2022-03-13T09:22:11.238011Z",
     "shell.execute_reply.started": "2022-03-13T09:22:05.909329Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e8b7b8af774c7ba984fa2bd6adca67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2892 rows loaded to df_spark_cities\n",
      "55075 rows loaded to df_spark_airport\n",
      "3096313 rows loaded to df_spark_immigration\n",
      "True"
     ]
    }
   ],
   "source": [
    "def check_count_dataset_to_spark(df_spark, df_spark_name):\n",
    "    \n",
    "    \"\"\" This function checks the number of rows loaded from dataset files/folder to spark dataframe\"\"\"\n",
    "    \n",
    "    numrows = df_spark.count()\n",
    "    if numrows > 0:\n",
    "        print(f\"{numrows} rows loaded to {df_spark_name}\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "check_count_dataset_to_spark(df_spark_cities, 'df_spark_cities')\n",
    "check_count_dataset_to_spark(df_spark_airport, 'df_spark_airport')\n",
    "check_count_dataset_to_spark(df_spark_immigration, 'df_spark_immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T10:03:04.612321Z",
     "iopub.status.busy": "2022-03-13T10:03:04.612089Z",
     "iopub.status.idle": "2022-03-13T10:03:08.204715Z",
     "shell.execute_reply": "2022-03-13T10:03:08.132839Z",
     "shell.execute_reply.started": "2022-03-13T10:03:04.612298Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f706949b164fa4a5062dca6576efb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2892 rows loaded from to cities_table\n",
      "55075 rows loaded from to airport_table\n",
      "3096313 rows loaded from to immigrant_table\n",
      "3096313 rows loaded from to immigration_table\n",
      "True"
     ]
    }
   ],
   "source": [
    "def check_count_table_to_s3(df_spark, table_name):\n",
    "    \n",
    "    \"\"\" This function checks the number of rows loaded from tables to table         \n",
    "    \"\"\"\n",
    "    num_rows = df_spark.count()\n",
    "    if num_rows > 0:\n",
    "        print(f\"{num_rows} rows loaded from to {table_name}\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "check_count_table_to_s3(df_spark_cities, 'cities_table')\n",
    "check_count_table_to_s3(df_spark_airport, 'airport_table')\n",
    "check_count_table_to_s3(df_spark_immigration, 'immigrant_table')\n",
    "check_count_table_to_s3(df_spark_immigration, 'immigration_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "#### Dimension tables:\n",
    "##### cities_table\n",
    "        city_id :    Primary key of city table obtained by md5(city||state_name)     \n",
    "        city :       The name of the city\n",
    "        state_name:  the full name of the state\n",
    "        state_code:  The state code which is made of the two first letters of the state name \n",
    "        median_age:  The median age in the city \n",
    "        male_population:   total count of male population \n",
    "        female_population: total count of female population\n",
    "        total_population:  total population of the city\n",
    "           \n",
    "##### airport_table\n",
    "        airport_id:   Primary key of the airport which is also the local code of the airport\n",
    "        airport_type: contains the size type of the airport\n",
    "        airport_name: contains the name of the airport\n",
    "        elevation_ft: contains the elevation foot of the airport\n",
    "        state_code:   contains the state code where the airport is located\n",
    "        city :        The name of the city where the airport is located\n",
    "        latitude:     extracted from coordinates column, contains the latitude of the coordinates\n",
    "        longitude:    extracted from coordinates column, contains the latitude of the coordinates                                                      \n",
    "           \n",
    "##### immigrant_table\n",
    "        imm_id:     Primary key of the immigrant_table, contains the immigrant identifier\n",
    "        age:        contains the age of the immigrant\n",
    "        birth_year: contains the birth year of the immigrant\n",
    "        gender:     contains the gender of the immigrant\n",
    "        visa_type:  contains the visa type of the immigrant\n",
    "        state_code: contains the state code where the immigrant is located. which is made of the two first letters of the state name       \n",
    "           \n",
    "#### Fact table:\n",
    "##### immigration_table\n",
    "        imm_id:        Primary key of the immigration_table, and also the identifier of immigrant_table contains the immigration identifier  \n",
    "        airport_id:    Foreign Key, is the identifier of the airport_table which is aloso th local code of the airport\n",
    "        city_id:       Foreign Key, is the identifier of cities_table obtained by md5(city||state_name)\n",
    "        arrival_date:   Contains the arrival date converted from SAS format to datetime format\n",
    "        departure_date: Contains the departure date converted from SAS format to datetime format\n",
    "        mode:           Contains the mode of arrival. Either sea, air, Land or Not reported\n",
    "        state_code:     contains the state code where the immigration is done. which is made of the two first letters of the state name\n",
    "        immigration_year:    contains the immigration year\n",
    "        immigration_month:    contains the immigration month\n",
    "        stay_duration_in_days:  contains the immigration duration in terms of number days\n",
    "        stay_duration_in_weeks:  contains the immigration duration in terms of number weeks\n",
    "        stay_duration_in_months:  contains the immigration duration in terms of number months\n",
    "        stay_duration_in_years:  contains the immigration duration in terms of number years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "#### Rational for the choice of tools and technologies for the project.\n",
    "#### PySpark running on AWS Elastic Map Reduce (EMR) cluster: \n",
    "\n",
    "* To modelize structured and un/semi-structured datasets, to Extract Transform and Load data in a set of tables with customizable schemas. Suitable for large data processing. Has an API with Pandas and cutting-edge query features such as SQL.\n",
    "\n",
    "#### AWS S3:\n",
    "\n",
    "* Object storage service offering industry-leading scalability, data availability, security, and performance.\n",
    "\n",
    "#### Pandas: \n",
    "\n",
    "* For sample dataset exploratory, data analysis is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python         programming language.\n",
    "\n",
    "\n",
    "#### Data update and why.\n",
    "* Tables created from immigration data set should be updated monthly since the raw data set is built up monthly.\n",
    "* Tables created from demography data set could be updated annually since demography data collection takes time and high frequent demography might take high cost but generate wrong conclusion.\n",
    "* Tables created from airport data could be updated every five years since airport data are not much dynamic\n",
    "* All tables should be update in an append-only mode.\n",
    "\n",
    "\n",
    "#### Approach of the problem differently under the following scenarios:\n",
    "\n",
    "##### The data was increased by 100x.\n",
    "\n",
    "* If Spark with standalone server mode can not process 100x data set, the appropriate solution is to rationaly scale the capacity in terms of number of nodes, memory and network of the EMR cluster, which is a distributed data cluster for processing large data sets on cloud.\n",
    " \n",
    "##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "* Apache Airflow could be used for building up a ETL data pipeline to be triggered regular interval and update the date and populate a report. Airflow provides many plug-and-play operators that are ready to execute desired tasks and deliever more powerful task automation.\n",
    "   \n",
    "##### The database needed to be accessed by 100+ people.\n",
    "\n",
    "* AWS Redshift database can handle hundreds of connections.Massively parallel processing,Columnar data storage, Query optimizer, Result caching, are the characteristics of redshift that make it a powerful technology to handle many connections. Moving the database to Redshift will be a rational choice to envisage.\n",
    "\n",
    " \n",
    "#### Evidence that the ETL processes results in the data model \n",
    "Below is a SparkSQL query that includes multiple JOIN statements on all of the four tables of our data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T10:03:26.362249Z",
     "iopub.status.busy": "2022-03-13T10:03:26.361989Z",
     "iopub.status.idle": "2022-03-13T10:14:13.542414Z",
     "shell.execute_reply": "2022-03-13T10:14:13.541726Z",
     "shell.execute_reply.started": "2022-03-13T10:03:26.362222Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f520c98d704e9f90c40bd80015a06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+------------+--------------+----+----------+----------------+-----------------+------------------+------------------+----+\n",
      "|imm_id|airport_id|             city_id|arrival_date|departure_date|mode|state_code|immigration_year|immigration_month|          latitude|         longitude|visa|\n",
      "+------+----------+--------------------+------------+--------------+----+----------+----------------+-----------------+------------------+------------------+----+\n",
      "|646322|      00FD|239a1b21f5bf8dab2...|       20548|         20571| sea|        FL|            2016|                4|-82.34539794921875|28.846599578857422|  B1|\n",
      "|522855|      00FD|239a1b21f5bf8dab2...|       20547|         20553| air|        FL|            2016|                4|-82.34539794921875|28.846599578857422|  B2|\n",
      "|620141|      00FD|239a1b21f5bf8dab2...|       20547|         20554| air|        FL|            2016|                4|-82.34539794921875|28.846599578857422|  B2|\n",
      "+------+----------+--------------------+------------+--------------+----+----------+----------------+-----------------+------------------+------------------+----+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "evidence_table = spark.sql(\"\"\"SELECT int(immigration_table.cicid) imm_id,\n",
    "                                               airport_table.ident airport_id,                                     \n",
    "                                               md5(city||state_name) city_id,                                     \n",
    "                                               int(immigration_table.arrdate) arrival_date,\n",
    "                                               int(immigration_table.depdate) departure_date,\n",
    "                                               CASE WHEN int(immigration_table.i94mode)=1 THEN \"air\"\n",
    "                                                    WHEN int(immigration_table.i94mode)=2 THEN \"sea\"\n",
    "                                                    WHEN int(immigration_table.i94mode)=3 THEN \"Land\" \n",
    "                                                    ELSE \"Not reported\" \n",
    "                                                    END as mode,\n",
    "                                               immigration_table.i94addr state_code,\n",
    "                                               int(immigration_table.i94yr) immigration_year,\n",
    "                                               int(immigration_table.i94mon) immigration_month,\n",
    "                                               airport_table.latitude,\n",
    "                                               airport_table.longitude,\n",
    "                                               immigrant_table.visatype visa\n",
    "                                               \n",
    "                                                                           \n",
    "                                          FROM  immigration_table\n",
    "                                          JOIN  immigrant_table\n",
    "                                          ON    immigration_table.cicid=immigrant_table.cicid\n",
    "                                          JOIN  airport_table\n",
    "                                          ON    immigrant_table.i94addr=airport_table.iso_region\n",
    "                                          JOIN  cities_table\n",
    "                                          ON    airport_table.municipality=cities_table.City AND airport_table.iso_region=cities_table.state_code                           \n",
    "                               \n",
    "                                        \"\"\").dropDuplicates().dropna()   \n",
    "        \n",
    "evidence_table.show(n=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
